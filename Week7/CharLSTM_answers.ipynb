{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CharLSTM_answers.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"AXoRJjuM2-fX","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","import sys\n","import math\n","import time\n","import itertools\n","\n","import tensorflow as tf\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from tensorflow import keras\n","from sklearn.preprocessing import OneHotEncoder\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E2mxrCHGXQyx","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"ln7sg112XQ9c","colab_type":"text"},"cell_type":"markdown","source":["# Recurrent Neural Networks\n","\n","[Karpathy's blog about RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"]},{"metadata":{"id":"R21p4AwhXvfB","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","    'If training vanilla neural nets is optimization over functions, training recurrent nets is optimization over programs.'"]},{"metadata":{"id":"0opiBUGLOXlh","colab_type":"text"},"cell_type":"markdown","source":["Sequences. Depending on your background you might be wondering: What makes Recurrent Networks so special? A glaring limitation of Vanilla Neural Networks (and also Convolutional Networks) is that their API is too constrained: they accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector as output (e.g. probabilities of different classes). Not only that: These models perform this mapping using a fixed amount of computational steps (e.g. the number of layers in the model). The core reason that recurrent nets are more exciting is that they allow us to operate over sequences of vectors: Sequences in the input, the output, or in the most general case both."]},{"metadata":{"id":"zFzU6EieXpUR","colab_type":"text"},"cell_type":"markdown","source":["![](http://karpathy.github.io/assets/rnn/diags.jpeg)\n","\n","Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNN's state. From left to right: \n","\n","1. Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). \n","\n","2. Sequence output (e.g. image captioning takes an image and outputs a sentence of words). \n","\n","3. Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). \n","\n","4. Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French).\n","\n","5. Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like."]},{"metadata":{"id":"IVlv6p8OYml8","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"0bLe8FITX7S9","colab_type":"text"},"cell_type":"markdown","source":["## RNN\n","\n","![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n","\n","![](https://image.slidesharecdn.com/rnn-lstm-161106132927/95/understanding-rnn-and-lstm-4-638.jpg?cb=1478439617)\n"]},{"metadata":{"id":"CmuzjCN9YnQU","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"y9vqvfb7Yd77","colab_type":"text"},"cell_type":"markdown","source":["## The Problem of Long-Term Dependencies\n","\n","![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png)\n","\n","![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png)"]},{"metadata":{"id":"DXLul8XgYn8_","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"0zaSfKBtYKV6","colab_type":"text"},"cell_type":"markdown","source":["## LSTM\n","\n","[Colah's blog about LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n","\n","![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n","\n","![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png)"]},{"metadata":{"id":"RxB6cCU3ZAji","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"stgYnrxEZArj","colab_type":"text"},"cell_type":"markdown","source":["## Language models\n","\n","![](https://raw.githubusercontent.com/torch/torch.github.io/master/blog/_posts/images/rnnlm.png)\n","\n","![](http://karpathy.github.io/assets/rnn/charseq.jpeg)\n","\n","![](https://i.redd.it/cw04e9546gv11.png)"]},{"metadata":{"id":"xcI2Ow7z2-h9","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"0rJ_JEMUZ6bV","colab_type":"text"},"cell_type":"markdown","source":["# Train character level RNN for language modelling"]},{"metadata":{"id":"w9-7eypD2-ul","colab_type":"text"},"cell_type":"markdown","source":["## Load corpora\n","\n","Upload text file from your drive"]},{"metadata":{"id":"ZnQ6gaXM2Qkf","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import files"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XYet8joa2wnp","colab_type":"code","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":"OK"}},"base_uri":"https://localhost:8080/","height":78},"outputId":"2e914131-1c6e-4286-b315-c0d54f18acb0","executionInfo":{"status":"ok","timestamp":1554966023611,"user_tz":-120,"elapsed":38210,"user":{"displayName":"Łukasz Maziarka","photoUrl":"","userId":"07217688615233494482"}}},"cell_type":"code","source":["uploaded = files.upload()"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-1ea4f4fc-ecbc-4215-82e8-e64e65842d7c\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-1ea4f4fc-ecbc-4215-82e8-e64e65842d7c\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving shakespeare.txt to shakespeare.txt\n"],"name":"stdout"}]},{"metadata":{"id":"vakZ99Wf2zLY","colab_type":"code","colab":{}},"cell_type":"code","source":["data = str(list(uploaded.values())[0])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rskbQq-a3BoT","colab_type":"text"},"cell_type":"markdown","source":["## Prepare dataset"]},{"metadata":{"id":"lndmged3bWdw","colab_type":"text"},"cell_type":"markdown","source":["Calculate vocabulary size. Create char2index and index2char dictionaries, that could help us in text vectorizing"]},{"metadata":{"id":"6teZDS3K28KZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8d184c24-a98d-4c7d-9abf-1b60a7049930","executionInfo":{"status":"ok","timestamp":1554966034241,"user_tz":-120,"elapsed":717,"user":{"displayName":"Łukasz Maziarka","photoUrl":"","userId":"07217688615233494482"}}},"cell_type":"code","source":["chars = list(set(data))\n","data_size, vocab_size = len(data), len(chars)\n","print('data has %d characters, %d unique.' % (data_size, vocab_size))\n","\n","char_to_ix = { ch:i for i,ch in enumerate(chars) }\n","ix_to_char = { i:ch for i,ch in enumerate(chars) }"],"execution_count":5,"outputs":[{"output_type":"stream","text":["data has 1155399 characters, 66 unique.\n"],"name":"stdout"}]},{"metadata":{"id":"E7ADrFEab2SZ","colab_type":"text"},"cell_type":"markdown","source":["Prepare text x and y datasets"]},{"metadata":{"id":"P0VZP6MP28Mf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b2190dbc-ddea-484b-89a6-d7f804e8d42e","executionInfo":{"status":"ok","timestamp":1554966035777,"user_tz":-120,"elapsed":1283,"user":{"displayName":"Łukasz Maziarka","photoUrl":"","userId":"07217688615233494482"}}},"cell_type":"code","source":["max_len = 20\n","step = 3\n","\n","sentences = []\n","next_chars = []\n","for i in range(0, len(data) - max_len, step):\n","    sentences.append(data[i: i + max_len])\n","    next_chars.append(data[i + max_len])\n","    \n","print(\"total # of sentences: \", len(sentences))    "],"execution_count":6,"outputs":[{"output_type":"stream","text":["total # of sentences:  385127\n"],"name":"stdout"}]},{"metadata":{"id":"0SeJxVjucBUL","colab_type":"text"},"cell_type":"markdown","source":["Translate string datasets into number vectors"]},{"metadata":{"id":"uFyWM42cDIwK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b700b0fc-155c-41c3-f7ad-f8383f7ccb21","executionInfo":{"status":"ok","timestamp":1554966041023,"user_tz":-120,"elapsed":3433,"user":{"displayName":"Łukasz Maziarka","photoUrl":"","userId":"07217688615233494482"}}},"cell_type":"code","source":["x = np.zeros((len(sentences), max_len), dtype=np.int)\n","y = np.zeros((len(sentences)), dtype=np.int)\n","\n","for i, sentence in enumerate(sentences):\n","    for t, char in enumerate(sentence):\n","        x[i, t] = char_to_ix[char]\n","        \n","    y[i] = char_to_ix[next_chars[i]]\n","    \n","x.shape, y.shape"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((385127, 20), (385127,))"]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"iuX-XQro5MvH","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"ctXdMbRX8PLS","colab_type":"text"},"cell_type":"markdown","source":["## Define model"]},{"metadata":{"id":"0niLOfSeeS9j","colab_type":"text"},"cell_type":"markdown","source":["Define the character level LSTM model for text generation that consists of:\n","\n","1.   (Optional) Embedding layer [keras.layers.Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) for training the word embeddings. You should pass the propper input dim to the layer and specify the embedding dim. **Note** that because we use the RNN model, you don't need to specify the input sequence length.\n","\n","2.   Some LSTM layers [keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) with specified number of hidden units.  **Note** that middle LSTM layers should return full sequences (you can specify this with parameter **return_sequences**).\n","\n","3.   Together with LSTM layers, you can also use the dropout layers [keras.layers.Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout), to regularize the network.\n","\n","4.   Final dense layer for making the classification [keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), with specified number of output units and activation function."]},{"metadata":{"id":"VOdCCGE9cuWZ","colab_type":"text"},"cell_type":"markdown","source":["**Define model hyperparameters**"]},{"metadata":{"id":"gACUhZX89STd","colab_type":"code","colab":{}},"cell_type":"code","source":["hidden_dim = 256\n","dropout = 0.2\n","\n","input_dim = vocab_size\n","output_dim = vocab_size"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eTRHsX6RcyLZ","colab_type":"text"},"cell_type":"markdown","source":["**Define model as keras sequential**"]},{"metadata":{"id":"ZErwu9Tvc5hI","colab_type":"code","colab":{}},"cell_type":"code","source":["model = keras.models.Sequential()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vC3TVHZgc16R","colab_type":"text"},"cell_type":"markdown","source":["**Define the embedding layer**\n","\n","You could define the **optional** embedding layer"]},{"metadata":{"id":"YC3S0Y_Kc1P-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"e539567c-a598-4362-ada2-5164d8f0c661","executionInfo":{"status":"ok","timestamp":1554966044260,"user_tz":-120,"elapsed":540,"user":{"displayName":"Łukasz Maziarka","photoUrl":"","userId":"07217688615233494482"}}},"cell_type":"code","source":["model.add(keras.layers.Embedding(vocab_size, hidden_dim))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"}]},{"metadata":{"id":"tQmwPFIvdC1I","colab_type":"text"},"cell_type":"markdown","source":["**Define LSTM layers**\n","\n","You could define as many LSTM layers, as you want. Remember that middle LSTM layers should return full sequences, not one word (you can specify this with parameter *return_sequences*). You could also define dropout after LSTM layers."]},{"metadata":{"id":"Rl_Q47SUdei0","colab_type":"code","colab":{}},"cell_type":"code","source":["model.add(keras.layers.LSTM(hidden_dim, return_sequences=True))\n","# model.add(keras.layers.Dropout(dropout))\n","\n","model.add(keras.layers.LSTM(hidden_dim, return_sequences=True))\n","# model.add(keras.layers.Dropout(dropout))\n","\n","model.add(keras.layers.LSTM(hidden_dim, return_sequences=False))\n","# model.add(keras.layers.Dropout(dropout))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"do60Rn_fdn5v","colab_type":"text"},"cell_type":"markdown","source":["**Define dense classification layer**"]},{"metadata":{"id":"aultpz_N5Mxs","colab_type":"code","colab":{}},"cell_type":"code","source":["model.add(keras.layers.Dense(output_dim, activation='softmax'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ij2SvklHc1TG","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"2Ir96n75d1WR","colab_type":"text"},"cell_type":"markdown","source":["**Compile the model**"]},{"metadata":{"id":"gacrPMqh_N4u","colab_type":"code","colab":{}},"cell_type":"code","source":["model.compile(loss='sparse_categorical_crossentropy', \n","              optimizer='adam',\n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bTHfUgI5dy4s","colab_type":"text"},"cell_type":"markdown","source":["**Check the model summary**"]},{"metadata":{"id":"JwQtJhYG8Qt9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":312},"outputId":"95f72d18-67b8-4422-e825-cf6b2d81071c","executionInfo":{"status":"ok","timestamp":1554966061933,"user_tz":-120,"elapsed":902,"user":{"displayName":"Łukasz Maziarka","photoUrl":"","userId":"07217688615233494482"}}},"cell_type":"code","source":["model.summary()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, None, 256)         16896     \n","_________________________________________________________________\n","lstm (LSTM)                  (None, None, 256)         525312    \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, None, 256)         525312    \n","_________________________________________________________________\n","lstm_2 (LSTM)                (None, 256)               525312    \n","_________________________________________________________________\n","dense (Dense)                (None, 66)                16962     \n","=================================================================\n","Total params: 1,609,794\n","Trainable params: 1,609,794\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"UO0EyyCxaEMd","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"0S3U-ac5B1mM","colab_type":"text"},"cell_type":"markdown","source":["## Define sampling function"]},{"metadata":{"id":"9L6Bo3ZOgIBD","colab_type":"text"},"cell_type":"markdown","source":["Define the function that takes the model and generates the string from characters sampled from model probabilities."]},{"metadata":{"id":"R7oxVKgoB356","colab_type":"code","colab":{}},"cell_type":"code","source":["def sample_string(model, seq_len, input_sequence=None):\n","    if input_sequence is None:\n","        generated_sequence = ix_to_char[x[np.random.choice(len(x)),0]]\n","    else:\n","        generated_sequence = input_sequence\n","    \n","    for _ in range(seq_len):\n","        seq_vector = np.array([[char_to_ix[c] for c in generated_sequence]])\n","        word_proba = model.predict_proba(seq_vector)\n","        predicted_word = ix_to_char[np.random.choice(vocab_size, p=word_proba.flatten())]\n","        generated_sequence += predicted_word\n","\n","    return generated_sequence"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yTo3KMEkaFbN","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"yzfXZe8dB4Af","colab_type":"text"},"cell_type":"markdown","source":["## Train model"]},{"metadata":{"id":"mOY8lJiI8QxM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1392},"outputId":"9fbea564-c596-4f61-8482-62909019cd7a"},"cell_type":"code","source":["sample_len = 100\n","\n","generated_string = sample_string(model, sample_len)\n","print(\"Sample string before training: '%s'\" % generated_string)\n","\n","for epoch in range(100):\n","    model.fit(x, y, batch_size=256, epochs=1)\n","    generated_string = sample_string(model, sample_len)\n","    print(\"Sample string after epoch %d: '%s'\" % (epoch, generated_string))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Sample string before training: 'olVh!ZUs\\SBtzWZha!l:wd-$$GZ.r?IKk\\g;&yE';HlsCiLglOXCZy\\mECeniy.lQ$rDuCxKTT:UBOySUe3vxLuL -AqmJMTuBkgB'\n","385127/385127 [==============================] - 245s 636us/sample - loss: 2.2159 - acc: 0.3705\n","Sample string after epoch 0: 'apint subelands.\\n\\nLELDTIAO:\\nSo, Vort whree with in resent ivense queentor, and so sigh is to fildo'\n","385127/385127 [==============================] - 243s 632us/sample - loss: 1.6187 - acc: 0.5176\n","Sample string after epoch 1: ' lade a bard.\\n\\nGRUMIO:\\nI have pands toe prymal mery, and but not mands.\\n\\nCORTILIO:\\nBut fecal up'\n","385127/385127 [==============================] - 243s 631us/sample - loss: 1.4648 - acc: 0.5559\n","Sample string after epoch 2: 'vay; the in your evortess.\\n\\nLADWE KISGER IF CAURERC:\\nLemger, as the mowiney, but harder! now in it'\n","385127/385127 [==============================] - 244s 633us/sample - loss: 1.3819 - acc: 0.5763\n","Sample string after epoch 3: 'rie':\\nMethink with drops the maid, and whate they wak.\\n\\nMARCIUS:\\nMy nobles will be the deventer H'\n","385127/385127 [==============================] - 251s 653us/sample - loss: 1.3258 - acc: 0.5903\n","Sample string after epoch 4: 'nSOLKST:\\nWhere is our noble and Northumbe, lost ten no husband proncixed. Come gent no garmingmented'\n","385127/385127 [==============================] - 255s 661us/sample - loss: 1.2807 - acc: 0.6022\n","Sample string after epoch 5: 'told us\\nNor tyrilly going, and quist him fail of us Namentius of knave?\\n\\nPAULINA:\\nCate? there's f'\n","385127/385127 [==============================] - 248s 645us/sample - loss: 1.2418 - acc: 0.6114\n","Sample string after epoch 6: 'dhis write meaning:\\nBe on socied very Coriolanus heiring to boy.\\nNow hath the strives me as I did g'\n","385127/385127 [==============================] - 244s 634us/sample - loss: 1.2053 - acc: 0.6211\n","Sample string after epoch 7: 'oGwroy.\\n\\nNurse:\\nGo tell the fatch in how child, I must believe them?\\n\\nSICINIUS:\\nHave for I beho'\n","385127/385127 [==============================] - 244s 634us/sample - loss: 1.1702 - acc: 0.6311\n","Sample string after epoch 8: ' pursuish,\\nI' seed a ploberty!\\n\\nPOMPEY:\\nHere's at it, she is thee, Marcius,\\nThou struck me, and '\n","385127/385127 [==============================] - 243s 631us/sample - loss: 1.1358 - acc: 0.6407\n","Sample string after epoch 9: '\\QQaH:\\nNo, I'll not think.\\n\\nISABELLA:\\nWhat feel'd in Kurships of shalbing winters but that meant '\n","385127/385127 [==============================] - 246s 638us/sample - loss: 1.1016 - acc: 0.6504\n","Sample string after epoch 10: 'aQQQ3hQ?DQQQG:3JGparror.\\n\\nGREGORY:\\nAy; but do you say you have time, ere I; therefore; be that I\\n'\n","385127/385127 [==============================] - 244s 635us/sample - loss: 1.0665 - acc: 0.6608\n","Sample string after epoch 11: 'nRKWO KI:\\n\\nHASTINGS:\\nWhich a pleasant sword have O will not jest under my bloody brother Claudio h'\n","385127/385127 [==============================] - 246s 638us/sample - loss: 1.0313 - acc: 0.6702\n","Sample string after epoch 12: 'e3nay,ddoom stemmn'd the meselate to see. Thy cause, my treads, doth no fiend come brow these violenc'\n","385127/385127 [==============================] - 251s 651us/sample - loss: 0.9959 - acc: 0.6816\n","Sample string after epoch 13: ' atca?\\n\\nGLOUCESTER:\\nIf she seek you look'd you to your own?\\n\\nWidow:\\nSo will make her and but tr'\n","385127/385127 [==============================] - 256s 664us/sample - loss: 0.9608 - acc: 0.6917\n","Sample string after epoch 14: 'iggeths live,\\nI will rejoy it is.\\n\\nCORIOLANUS:\\nI know nor, here; I'll gill me that hand in this f'\n","385127/385127 [==============================] - 254s 659us/sample - loss: 0.9271 - acc: 0.7031\n","Sample string after epoch 15: 'ycrair;\\nAs for my teque strong and distrack'd stuff\\nAnd stands she stolp, between usurps he execute'\n","385127/385127 [==============================] - 247s 641us/sample - loss: 0.8923 - acc: 0.7132\n","Sample string after epoch 16: 'i?QHTWratchian drop his men to o'd us to our way prisoner down,\\nMethinks you this banish'd years o' '\n","385127/385127 [==============================] - 244s 634us/sample - loss: 0.8597 - acc: 0.7242\n","Sample string after epoch 17: 'mawell.\\n\\nLEONTES:\\nWhy, that we mound this in strength in queen:\\nWhen, Tittinous ope Tybalt aims w'\n","385127/385127 [==============================] - 245s 637us/sample - loss: 0.8288 - acc: 0.7329\n","Sample string after epoch 18: 'e3QQRCMI:\\nI do not have mercy dance, if he with deformity that I see them incensed hither. He that n'\n","385127/385127 [==============================] - 245s 637us/sample - loss: 0.7991 - acc: 0.7425\n","Sample string after epoch 19: 'uQQQQNN:\\nNay, that's\\nYour purpose, being thee all rove to be value taste.\\nBut my greatery bondy ma'\n","385127/385127 [==============================] - 247s 641us/sample - loss: 0.7716 - acc: 0.7515\n","Sample string after epoch 20: 'sapparent:\\nThese welcomes not to make good with hold in celterity?\\n\\nProvost:\\nGive me your general'\n","385127/385127 [==============================] - 246s 639us/sample - loss: 0.7448 - acc: 0.7595\n","Sample string after epoch 21: 'uQQQQQ:\\QI and I'll content 'tis a thoughts to his whither staying up this heart? Come, life!\\nAnd wa'\n","385127/385127 [==============================] - 252s 654us/sample - loss: 0.7193 - acc: 0.7676\n","Sample string after epoch 22: 'aQQQQQQQL:\\nThe queen hath too common back?\\nHadst thou not promise her and beyound out of compasses '\n","385127/385127 [==============================] - 251s 651us/sample - loss: 0.6972 - acc: 0.7740\n","Sample string after epoch 23: ':\\nNay! the good father's bound, our solemnity to aguxe he dream that late spurate; for Ixen at the t'\n","385127/385127 [==============================] - 247s 640us/sample - loss: 0.6745 - acc: 0.7816\n","Sample string after epoch 24: ' courtpy;\\nAs, and pranks for court-hory too; and my bed, the wars from you\\nThat I can confess it go'\n","385127/385127 [==============================] - 244s 634us/sample - loss: 0.6566 - acc: 0.7869\n","Sample string after epoch 25: 'rayy!nAh, for his gracious lord, it would not a noted you to have broke from my son? This proclaim an'\n","385127/385127 [==============================] - 242s 629us/sample - loss: 0.6377 - acc: 0.7924\n","Sample string after epoch 26: 'rz I pray,\\nYou cannot here here reson your parents and citizens to her poor prescize and never valum'\n","385127/385127 [==============================] - 244s 633us/sample - loss: 0.6199 - acc: 0.7986\n","Sample string after epoch 27: 'dinung religitu,\\nHow cercums the enfordilant was the shadow upon the mind with his way with the mort'\n","385127/385127 [==============================] - 246s 639us/sample - loss: 0.6059 - acc: 0.8029\n","Sample string after epoch 28: ' quome you? The tempers to me but then?\\n\\nKING RICHARD III:\\nRomeo, at our soldivy blacks, our frien'\n","385127/385127 [==============================] - 244s 633us/sample - loss: 0.5926 - acc: 0.8071\n","Sample string after epoch 29: 'o,D\\nHabb.\\n\\nLADY ANNE:\\nHow fares a deer-whither way I will ne'er and thy neble sudden people?\\n\\nN'\n","385127/385127 [==============================] - 241s 626us/sample - loss: 0.5817 - acc: 0.8093\n","Sample string after epoch 30: ' get?\\n\\nKING RICHARD III:\\nIf thou let's since I did indegh your sovereigns strange flemen of mine, '\n","385127/385127 [==============================] - 243s 630us/sample - loss: 0.5697 - acc: 0.8132\n","Sample string after epoch 31: 'op;\\nGother, good sister, let ender been a suit the house of your lebelard, Henry honest--as stranged'\n","385127/385127 [==============================] - 251s 651us/sample - loss: 0.5616 - acc: 0.8155\n","Sample string after epoch 32: 'speck:\\nYour subject, for a king, out of our scours, I cannot we blaze of Englas strong, and not love'\n","385127/385127 [==============================] - 243s 630us/sample - loss: 0.5510 - acc: 0.8183\n","Sample string after epoch 33: 'uQQQQQ:\\Q\\QRIQQQ:\\nMy lanuus Henry's silence, and to thy death and bidding? our captain us? Warwick's'\n","385127/385127 [==============================] - 241s 627us/sample - loss: 0.5449 - acc: 0.8206\n","Sample string after epoch 34: 'darry. Now, away, for learners, send and sorch the good king of proud heart!\\n\\nQUEEN ELIZABETH:\\nIt '\n","385127/385127 [==============================] - 241s 626us/sample - loss: 0.5387 - acc: 0.8224\n","Sample string after epoch 35: 'uQQNQ:\\nGrantly!\\nWill you do me depart from Place.\\n\\nJULIET:\\nAnd welcome her hence.\\n\\nMORAGIUS:\\n'\n","385127/385127 [==============================] - 239s 621us/sample - loss: 0.5304 - acc: 0.8245\n","Sample string after epoch 36: ' plimmes!' this discontent now.\\n\\nISABELLA:\\nSir, as I say, is else I live a Jyward's eldies, past, '\n","385127/385127 [==============================] - 241s 627us/sample - loss: 0.5228 - acc: 0.8267\n","Sample string after epoch 37: 'aQQ3QQQ3QQ:\\nMy Trespass Seest the ribbles practise of all his?\\nHow he must not be a maiden present '\n","250880/385127 [==================>...........] - ETA: 1:24 - loss: 0.4949 - acc: 0.8371Buffered data was truncated after reaching the output size limit."],"name":"stdout"}]},{"metadata":{"id":"c7-qVMOJI8uT","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"nyPKBNoFVvIE","colab_type":"text"},"cell_type":"markdown","source":["# Images sources\n","\n","Images and code fragments used in this notebook comes from the following web pages and papers:\n","\n","1. http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n","2. http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n","3. https://pt.slideshare.net/ssuser6c624f/understanding-rnn-and-lstm\n","4. [BERT paper](https://arxiv.org/pdf/1810.04805.pdf)\n","5. https://github.com/JY-H/character-level-rnn/blob/master/src/character_level.py"]},{"metadata":{"id":"J9qyFHWwXH7w","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}